{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SQLContext\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import ceil, unix_timestamp\n",
    "from pyspark.sql.functions import rank\n",
    "from pyspark.sql.functions import collect_list, array\n",
    "from pyspark.mllib.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from scipy.interpolate import interpolate\n",
    "from future.utils import lmap\n",
    "from functools import partial\n",
    "from scipy.signal import butter, filtfilt\n",
    "import numpy as np\n",
    "import pywt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"some_testing2\").master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('com.databricks.spark.csv').option(\"header\", \"True\").option(\"delimiter\", \",\")\\\n",
    "                      .load('C:/Users/awagner/Desktop/For_Tom/'+'AllLabData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"X\", df[\"X\"].cast(\"double\"))\n",
    "df = df.withColumn(\"Y\", df[\"Y\"].cast(\"double\"))\n",
    "df = df.withColumn(\"Z\", df[\"Z\"].cast(\"double\"))\n",
    "df = df.withColumn(\"TSStart\", df[\"TSStart\"].cast(\"timestamp\"))\n",
    "df = df.withColumn(\"TSEnd\", df[\"TSEnd\"].cast(\"timestamp\"))\n",
    "df = df.withColumn(\"interval_start\", ((ceil(unix_timestamp(df[\"TSStart\"]).cast(\"long\")))%10**4)) \n",
    "df = df.withColumn(\"interval_end\", ((ceil(unix_timestamp(df[\"TSEnd\"]).cast(\"long\")))%10**4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "schema_for_parse = ArrayType(FloatType(), False)\n",
    "\n",
    "find_milisec = udf(lambda raw: (raw)[(raw.find('.')+1):(raw.find('.')+3)])\n",
    "merge_integers = udf(lambda raw1, raw2: int(str(raw1) + str(raw2)))\n",
    "parse = udf(lambda s: eval(str(s)), schema_for_parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"temp\", find_milisec('TS')) \n",
    "df = df.withColumn(\"interval\", (((unix_timestamp(df[\"TS\"]).cast(\"long\"))))) \n",
    "df = df.withColumn(\"interval\", merge_integers('interval', 'temp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def give_my_key(*args):\n",
    "    key = 0\n",
    "    for i in args:\n",
    "        key += float(i)\n",
    "    return key\n",
    "\n",
    "give_my_key_udf = udf(give_my_key)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"key\", give_my_key_udf(\"interval_start\", \"interval_end\", 'SubjectId') ) \n",
    "df = df.withColumn(\"key\", df[\"key\"].cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('_c0', 'string'), ('SessionId', 'string'), ('DeviceID', 'string'), ('TS', 'string'), ('X', 'double'), ('Y', 'double'), ('Z', 'double'), ('AnnotationStrValue', 'string'), ('BradykinesiaGA', 'string'), ('DyskinesiaGA', 'string'), ('TremorGA', 'string'), ('TSStart', 'timestamp'), ('TSEnd', 'timestamp'), ('SubjectId', 'string'), ('IntelUsername', 'string'), ('interval_start', 'bigint'), ('interval_end', 'bigint'), ('temp', 'string'), ('interval', 'string'), ('key', 'double')]\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------+------------------+-------------------+\n",
      "|key   |X                     |Y                 |Z                  |\n",
      "+------+----------------------+------------------+-------------------+\n",
      "|1741.0|0.032816266377255404  |0.9565589334411009|0.18538697489378717|\n",
      "|1741.0|0.05317390375614024   |0.944447047479542 |0.18932435971451844|\n",
      "|1741.0|0.0449467726174725    |0.9766106438670463|0.14992291459228008|\n",
      "|1741.0|0.037274439836516335  |0.972530163064418 |0.1302699554976262 |\n",
      "|1741.0|0.02876682562714425   |0.9846718973546182|0.14208155157964325|\n",
      "|1741.0|0.06386329194215588   |1.0370491835109428|0.1301143953870914 |\n",
      "|1741.0|0.02292142675033916   |1.0574879639064827|0.15766355101803806|\n",
      "|1741.0|-0.02008564460674698  |1.0532871058391622|0.08293830677224615|\n",
      "|1741.0|0.030053131108873775  |0.9884257934881129|0.04776625405385437|\n",
      "|1741.0|-0.005639048934114398 |0.9562325486916411|0.0753371654395789 |\n",
      "|1741.0|0.013946962127489297  |0.9803962072112112|0.06745177037089355|\n",
      "|1741.0|-0.0025368052929393772|0.9966379397093545|0.06741675404962157|\n",
      "|1741.0|-0.01861460645172814  |1.0046985298101219|0.05947504678795665|\n",
      "|1741.0|-0.005446618429564426 |0.9602803267118343|0.05566380877417289|\n",
      "|1741.0|0.03305150831268097   |1.0006897277847213|0.09880290936889141|\n",
      "|1741.0|0.09933770614686876   |0.8995798278098202|0.11869126076695605|\n",
      "|1741.0|0.056934657210248915  |0.9564793710252143|0.18147013471973056|\n",
      "|1741.0|0.058387785603614906  |1.057546271983679 |0.19688448334397207|\n",
      "|1741.0|0.09176189094723262   |1.013028500655454 |0.18133173050667323|\n",
      "|1741.0|0.06490994284225073   |0.9524795482300452|0.1932161855909678 |\n",
      "|1741.0|0.09723298731369909   |0.948338555200605 |0.1853946725092217 |\n",
      "|1741.0|0.11163582173105177   |0.8753925352869759|0.1423221650273031 |\n",
      "|1741.0|0.07804186795899423   |0.9120704955721776|0.18548287636171146|\n",
      "|1741.0|0.09672107658149992   |0.9605744504857674|0.20101699067938336|\n",
      "|1741.0|0.08780940143753786   |1.0049203076709134|0.18917681753625126|\n",
      "+------+----------------------+------------------+-------------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['key', 'X', 'Y', 'Z']).show(25 , False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_test = df.select('key','X', 'Y', 'Z', 'interval').rdd.map\\\n",
    "                 (lambda raw: (raw[0],([raw[1]], [raw[2]],  [raw[3]],  [raw[4]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_test = rdd_test.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2], x[3] + y[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_raw= rdd_test.map(lambda row : (row[0] ,row[1][0], row[1][1], row[1][2], row[1][3])).\\\n",
    "                       toDF(['key', 'X', 'Y', 'Z', 'interval'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    key|                   X|                   Y|                   Z|            interval|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+\n",
      "|12590.0|[0.28632198706050...|[0.82812405663445...|[-0.4823275881834...|[141596621950, 14...|\n",
      "|13040.0|[0.92564838141756...|[-0.0091315938837...|[-0.3272873282107...|[141596643850, 14...|\n",
      "|13450.0|[0.89605250785223...|[-0.3789722799210...|[0.09013530686785...|[141596664450, 14...|\n",
      "|14670.0|[0.28530783043499...|[0.32870670526588...|[0.93734698900573...|[141596725550, 14...|\n",
      "|20000.0|[0.02633892612881...|[-1.0006468761304...|[-0.3797875468897...|[141596991950, 14...|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sort_vec = udf(lambda X, Y: [x for _,x in sorted(zip(Y,X))])\n",
    "\n",
    "df_raw = df_raw.withColumn('X', sort_vec('X', 'interval'))\n",
    "df_raw = df_raw.withColumn('Y', sort_vec('Y', 'interval'))\n",
    "df_raw = df_raw.withColumn('Z', sort_vec('Z', 'interval'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key', 'double'),\n",
       " ('X', 'string'),\n",
       " ('Y', 'string'),\n",
       " ('Z', 'string'),\n",
       " ('interval', 'array<string>')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_raw = df_raw.withColumn(\"X\",  parse(\"X\"))\n",
    "df_raw = df_raw.withColumn(\"Y\",  parse(\"Y\"))\n",
    "df_raw = df_raw.withColumn(\"Z\",  parse(\"Z\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slidind_window(axis, time_stamp, slide, window_size, freq):\n",
    "    #axis = eval(axis)\n",
    "    t = time_stamp[0]\n",
    "    windows = []\n",
    "    windows.append(axis[:(window_size*freq+1)])\n",
    "    for time1 in range(len(time_stamp)):\n",
    "        if float(time_stamp[time1]) >= float(t) + 100*slide:\n",
    "            if time1+window_size*freq < len(time_stamp):\n",
    "                windows.append(axis[time1:(time1+window_size*freq+1)])\n",
    "                t =  time_stamp[time1]\n",
    "    \n",
    "    return (windows)\n",
    "\n",
    "sliding_window_partial = partial(slidind_window, slide = 2.5, window_size = 5, freq = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "schema_for_sliding= ArrayType(ArrayType(FloatType(), False), False)\n",
    "sliding_window_udf = udf(sliding_window_partial, schema_for_sliding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_raw = df_raw.withColumn('X', sliding_window_udf('X', 'interval'))\n",
    "df_raw = df_raw.withColumn('Y', sliding_window_udf('Y', 'interval'))\n",
    "df_raw = df_raw.withColumn('Z', sliding_window_udf('Z', 'interval'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key', 'double'),\n",
       " ('X', 'array<array<float>>'),\n",
       " ('Y', 'array<array<float>>'),\n",
       " ('Z', 'array<array<float>>'),\n",
       " ('interval', 'array<string>')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = df_raw.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 56.0 failed 1 times, most recent failure: Lost task 0.0 in stage 56.0 (TID 62, localhost, executor driver): java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-9c28a8c2b275>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_raw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mraw\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;33m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_flat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatMapValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mraw\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_flat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mraw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'key'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'X'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Z'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\awagner\\Documents\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\awagner\\Documents\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\awagner\\Documents\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \"\"\"\n\u001b[1;32m    374\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\awagner\\Documents\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;32mclass\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \"\"\"\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[0;32mC:\\Users\\awagner\\Documents\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \"\"\"\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\awagner\\Documents\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\awagner\\Documents\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\awagner\\Documents\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\awagner\\Documents\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\awagner\\Documents\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 56.0 failed 1 times, most recent failure: Lost task 0.0 in stage 56.0 (TID 62, localhost, executor driver): java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "df_flat = df_raw.rdd.map(lambda raw:  (raw[0] , list(zip(raw[1], raw[2], raw[3]))))\n",
    "df_flat = df_flat.flatMapValues(lambda raw :raw)\n",
    "df_flat = df_flat.map(lambda raw: (raw[0],raw[1][0],raw[1][1],raw[1][2])).toDF(['key', 'X', 'Y', 'Z'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|     _1|                  _2|\n",
      "+-------+--------------------+\n",
      "|12590.0|[WrappedArray(0.2...|\n",
      "|12590.0|[WrappedArray(0.1...|\n",
      "|12590.0|[WrappedArray(0.1...|\n",
      "|12590.0|[WrappedArray(0.2...|\n",
      "|12590.0|[WrappedArray(0.3...|\n",
      "|12590.0|[WrappedArray(0.2...|\n",
      "|12590.0|[WrappedArray(0.2...|\n",
      "|13040.0|[WrappedArray(0.9...|\n",
      "|13040.0|[WrappedArray(0.9...|\n",
      "|13040.0|[WrappedArray(1.3...|\n",
      "|13040.0|[WrappedArray(0.8...|\n",
      "|13040.0|[WrappedArray(1.0...|\n",
      "|13040.0|[WrappedArray(1.2...|\n",
      "|13040.0|[WrappedArray(1.0...|\n",
      "|13040.0|[WrappedArray(1.0...|\n",
      "|13040.0|[WrappedArray(1.2...|\n",
      "|13040.0|[WrappedArray(1.0...|\n",
      "|13040.0|[WrappedArray(1.1...|\n",
      "|13040.0|[WrappedArray(0.9...|\n",
      "|13040.0|[WrappedArray(0.8...|\n",
      "|13040.0|[WrappedArray(1.1...|\n",
      "|13040.0|[WrappedArray(-0....|\n",
      "|13040.0|[WrappedArray(-0....|\n",
      "|13040.0|[WrappedArray(-0....|\n",
      "|13040.0|[WrappedArray(-1....|\n",
      "+-------+--------------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_flat.toDF().show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def project_gravity(x, y, z, num_samples_per_interval=None, round_up_or_down='down', return_only_vertical=False):\n",
    "    \"\"\"\n",
    "    Projection of 3D time signal to 2D\n",
    "    \n",
    "    Input:\n",
    "        x (1D numpy) - time signal X samples\n",
    "        y (1D numpy) - time signal Y samples\n",
    "        z (1D numpy) - time signal Z samples\n",
    "        num_samples_per_interval (integer) - cut the signal to num_samples_per_interval sub intervals\n",
    "                                     and preform the 2D projection on the sub intervals\n",
    "        round_up_or_down (string, down or up) - length(x)/num_samples_per_interval should \n",
    "                                                be ceil or floor\n",
    "        return_only_vertical (boolean) - If True return only vertical axis\n",
    "    \n",
    "    Output:\n",
    "        v (1D numpy) - vertical projection\n",
    "        h (1D numpy) - horizontal projection\n",
    "                                  \n",
    "    \"\"\"\n",
    "    if num_samples_per_interval is None:\n",
    "        v, h = project_gravity_xyz(x, y, z)\n",
    "        if return_only_vertical:\n",
    "            return v\n",
    "        else:\n",
    "            return v, h\n",
    "\n",
    "    # set number of intervals\n",
    "    n = len(x)/num_samples_per_interval\n",
    "    if round_up_or_down == 'down':\n",
    "        n = np.floor(n).astype(int)\n",
    "        n = np.max([1, n])\n",
    "    elif round_up_or_down == 'up':\n",
    "        n = np.ceil(n).astype(int)\n",
    "\n",
    "    # set window size\n",
    "    win_size = np.floor(len(x)/n).astype(int)\n",
    "\n",
    "    # perform sliding windows\n",
    "    idx_start = 0\n",
    "    v = []\n",
    "    h = []\n",
    "\n",
    "    # TODO Chunk the samples below evenly. Do this by dividing len(x) each time rather than the current implementation\n",
    "    for i in range(n):\n",
    "        idx_start = i * win_size\n",
    "        idx_end = (i + 1) * win_size\n",
    "        if i == n-1:  # last iteration\n",
    "            idx_end = -1\n",
    "        x_i = x[idx_start:idx_end]\n",
    "        y_i = y[idx_start:idx_end]\n",
    "        z_i = z[idx_start:idx_end]\n",
    "        ver_i, hor_i = project_gravity_xyz(x_i, y_i, z_i)\n",
    "        v.append(ver_i)\n",
    "        h.append(hor_i)\n",
    "    if return_only_vertical:\n",
    "        return np.hstack(v)\n",
    "    return np.hstack(v), np.hstack(h)\n",
    "\n",
    "\n",
    "def project_gravity_xyz(x, y, z):\n",
    "    xyz = np.stack((x, y, z), axis=1)\n",
    "    return project_gravity_core(xyz)\n",
    "\n",
    "\n",
    "def project_gravity_core(xyz):\n",
    "    \"\"\"\n",
    "    Projection of data set to 2 dim\n",
    "    \n",
    "    Input:\n",
    "        xyz (3d numpy array) - 0 dimension is number os samples, 1 dimension is length of signals\n",
    "                    and 2 dim is number of axis\n",
    "                    \n",
    "    Output:\n",
    "        ver (1d numpy) - Vertical projection\n",
    "        hor (1d numpy) - Horizontal projection\n",
    "        \n",
    "    \"\"\"\n",
    "    ver = []\n",
    "    hor = []\n",
    "    \n",
    "    # mean for each axis\n",
    "    G = [np.mean(xyz[:, 0]), np.mean(xyz[:, 1]), np.mean(xyz[:, 2])]\n",
    "    G_norm = G/np.sqrt(sum(np.power(G, 2)) + 0.0000001)\n",
    "    \n",
    "    # The projection is here\n",
    "    for i in range(len(xyz[:, 0])):\n",
    "        ver.append(float(np.dot([xyz[i, :]], G)))\n",
    "        hor.append(float(np.sqrt(np.dot(xyz[i, :]-ver[i]*G_norm, xyz[i, :]-ver[i]*G_norm))))\n",
    "        \n",
    "    ver = np.reshape(np.asarray(ver), len(ver))\n",
    "    return Vectors.dense(ver), Vectors.dense(hor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|    key|            proj_ver|            proj_hor|\n",
      "+-------+--------------------+--------------------+\n",
      "|12590.0|[0.96981579045316...|[0.17319772894329...|\n",
      "|12590.0|[0.95745235257911...|[0.19960889960922...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema_for_proj = StructType([\n",
    "    StructField(\"proj_ver\", VectorUDT(), False),\n",
    "    StructField(\"proj_hor\", VectorUDT(), False)\n",
    "])\n",
    "\n",
    "proj_func = udf(project_gravity_xyz, schema_for_proj)\n",
    "\n",
    "\n",
    "df_proj = df_flat['X', 'Y', 'Z', 'key'].withColumn('proj', proj_func(\"X\", \"Y\", \"Z\"))\n",
    "df_proj = df_proj.select('key',\n",
    "                 'proj.proj_ver', \n",
    "                 'proj.proj_hor')\n",
    "df_proj.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|    key|        denoised_ver|        denoised_hor|\n",
      "+-------+--------------------+--------------------+\n",
      "|12590.0|[0.09713335271840...|[0.10704233282524...|\n",
      "|12590.0|[0.07484846475425...|[0.16028882851580...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def denoise(data):\n",
    "    \"\"\"\n",
    "    Denoise the data with wavelet and\n",
    "    Input:\n",
    "        data - time signal\n",
    "    Output:\n",
    "        result - signal after denoising\n",
    "    \"\"\"\n",
    "    data = data - np.mean(data) + 0.1\n",
    "    WC = pywt.wavedec(data, 'sym8')\n",
    "    threshold = 0.01*np.sqrt(2*np.log2(256))\n",
    "    NWC = lmap(lambda x: pywt.threshold(x, threshold, 'soft'), WC)\n",
    "    result = pywt.waverec(NWC, 'sym8')\n",
    "    return  Vectors.dense(result)\n",
    "\n",
    "schema_for_denoise_func = ArrayType(FloatType(), False)\n",
    "denoise_func = udf(denoise,  VectorUDT())\n",
    "\n",
    "\n",
    "\n",
    "df_denoise = df_proj['proj_ver','proj_hor', 'key'].withColumn('denoised_ver',\n",
    "                    denoise_func(\"proj_ver\")).withColumn('denoised_hor',denoise_func(\"proj_hor\"))\n",
    "df_denoise = df_denoise.select('key', \"denoised_ver\", \"denoised_hor\") \n",
    "df_denoise.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    key|    rel_features_ver|   cont_features_ver|    rel_features_hor|   cont_features_hor|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+\n",
      "|12590.0|[0.0,0.05599426,0...|[3.469447E-17,0.0...|[0.0,0.08517329,0...|[5.551115E-17,0.1...|\n",
      "|12590.0|[2.220446E-16,0.0...|[7.8062556E-17,0....|[0.0,0.050331336,...|[9.020562E-17,0.0...|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def toDWT(sig, rel = False):\n",
    "\n",
    "        x = np.arange(0, len(sig))\n",
    "        f = interpolate.interp1d(x, sig)\n",
    "        xnew = np.arange(0, len(sig)-1, float(len(sig)-1)/2**np.ceil(np.log2(len(sig))))\n",
    "        ynew = f(xnew)\n",
    "        x = pywt.wavedec(ynew - np.mean(ynew), pywt.Wavelet('db1'), mode='smooth')\n",
    "                \n",
    "        J = len(x)\n",
    "        res = np.zeros(J)\n",
    "        for j in range(J):\n",
    "            res[j] = float(np.sqrt(np.sum(x[j]**2)))\n",
    "        if rel is True:\n",
    "            res = res/np.sum(res + 10**(-10))\n",
    "            res = (np.log(float(1)/(1-res)))\n",
    "        \n",
    "        final_res = []\n",
    "        for not_kill in np.asarray(res):\n",
    "            final_res.append(float(not_kill))\n",
    "        return final_res\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"F1\", FloatType(), False),\n",
    "    StructField(\"F2\", FloatType(), False),\n",
    "    StructField(\"F3\", FloatType(), False),\n",
    "    StructField(\"F4\", FloatType(), False),\n",
    "    StructField(\"F5\", FloatType(), False),\n",
    "    StructField(\"F6\", FloatType(), False),\n",
    "    StructField(\"F7\", FloatType(), False),\n",
    "    StructField(\"F8\", FloatType(), False),\n",
    "    StructField(\"F9\", FloatType(), False)\n",
    "])\n",
    "\n",
    "toDWT_relative = partial(toDWT, rel = True)\n",
    "toDWT_cont = partial(toDWT, rel = False)\n",
    "\n",
    "toDWT_relative_udf = udf(toDWT_relative,  schema)\n",
    "toDWT_cont_udf = udf(toDWT_cont,  schema)\n",
    "\n",
    "df_features = df_denoise[\"denoised_ver\", \"denoised_hor\", 'key'].withColumn('rel_features_ver', \n",
    "                        toDWT_relative_udf(\"denoised_ver\")).withColumn('cont_features_ver',\n",
    "                                          toDWT_cont_udf(\"denoised_ver\"))\n",
    "\n",
    "df_features = df_features[\"rel_features_ver\", \"cont_features_ver\", \"denoised_hor\", 'key'].withColumn('rel_features_hor', \n",
    "                        toDWT_relative_udf(\"denoised_hor\")).withColumn('cont_features_hor',\n",
    "                                          toDWT_cont_udf(\"denoised_hor\"))\n",
    "\n",
    "\n",
    "df_features = df_features.select('key', 'rel_features_ver', 'cont_features_ver',\n",
    "                                 'rel_features_hor', 'cont_features_hor')\n",
    "df_features.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_array(col):\n",
    "    def to_array_(v):\n",
    "        return v\n",
    "    return udf(to_array_, ArrayType(FloatType()))(col)\n",
    "\n",
    "ready_for_model = (df_features\n",
    "    .withColumn(\"rel_features_ver\", to_array(col(\"rel_features_ver\")))\n",
    "    .withColumn(\"cont_features_ver\", to_array(col(\"cont_features_ver\")))\n",
    "    .withColumn(\"rel_features_hor\", to_array(col(\"rel_features_hor\")))\n",
    "    .withColumn(\"cont_features_hor\", to_array(col(\"cont_features_hor\")))          \n",
    "    .select([\"key\"] + [col(\"rel_features_ver\")[i] for i in range(9)] + \n",
    "            [col(\"cont_features_ver\")[i] for i in range(9)] + \n",
    "            [col(\"rel_features_hor\")[i] for i in range(9)] +\n",
    "            [col(\"cont_features_hor\")[i] for i in range(9)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key', 'double'),\n",
       " ('rel_features_ver[0]', 'float'),\n",
       " ('rel_features_ver[1]', 'float'),\n",
       " ('rel_features_ver[2]', 'float'),\n",
       " ('rel_features_ver[3]', 'float'),\n",
       " ('rel_features_ver[4]', 'float'),\n",
       " ('rel_features_ver[5]', 'float'),\n",
       " ('rel_features_ver[6]', 'float'),\n",
       " ('rel_features_ver[7]', 'float'),\n",
       " ('rel_features_ver[8]', 'float'),\n",
       " ('cont_features_ver[0]', 'float'),\n",
       " ('cont_features_ver[1]', 'float'),\n",
       " ('cont_features_ver[2]', 'float'),\n",
       " ('cont_features_ver[3]', 'float'),\n",
       " ('cont_features_ver[4]', 'float'),\n",
       " ('cont_features_ver[5]', 'float'),\n",
       " ('cont_features_ver[6]', 'float'),\n",
       " ('cont_features_ver[7]', 'float'),\n",
       " ('cont_features_ver[8]', 'float'),\n",
       " ('rel_features_hor[0]', 'float'),\n",
       " ('rel_features_hor[1]', 'float'),\n",
       " ('rel_features_hor[2]', 'float'),\n",
       " ('rel_features_hor[3]', 'float'),\n",
       " ('rel_features_hor[4]', 'float'),\n",
       " ('rel_features_hor[5]', 'float'),\n",
       " ('rel_features_hor[6]', 'float'),\n",
       " ('rel_features_hor[7]', 'float'),\n",
       " ('rel_features_hor[8]', 'float'),\n",
       " ('cont_features_hor[0]', 'float'),\n",
       " ('cont_features_hor[1]', 'float'),\n",
       " ('cont_features_hor[2]', 'float'),\n",
       " ('cont_features_hor[3]', 'float'),\n",
       " ('cont_features_hor[4]', 'float'),\n",
       " ('cont_features_hor[5]', 'float'),\n",
       " ('cont_features_hor[6]', 'float'),\n",
       " ('cont_features_hor[7]', 'float'),\n",
       " ('cont_features_hor[8]', 'float')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ready_for_model.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'),\n",
       " ('SessionId', 'string'),\n",
       " ('DeviceID', 'string'),\n",
       " ('TS', 'string'),\n",
       " ('X', 'string'),\n",
       " ('Y', 'string'),\n",
       " ('Z', 'string'),\n",
       " ('AnnotationStrValue', 'string'),\n",
       " ('BradykinesiaGA', 'string'),\n",
       " ('DyskinesiaGA', 'string'),\n",
       " ('TremorGA', 'string'),\n",
       " ('TSStart', 'string'),\n",
       " ('TSEnd', 'string'),\n",
       " ('SubjectId', 'string'),\n",
       " ('IntelUsername', 'string')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = df_flat.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(A[0][1][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
